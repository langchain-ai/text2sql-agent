name: Tests with Results

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  setup:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]
    outputs:
      python-version: ${{ matrix.python-version }}
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install uv
      uses: astral-sh/setup-uv@v1
      with:
        version: "latest"
    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: |
          .uv/cache
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    - name: Install dependencies
      run: uv sync

  unit-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
      LANGSMITH_TRACING: ${{ secrets.LANGSMITH_TRACING }}
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install uv
      uses: astral-sh/setup-uv@v1
      with:
        version: "latest"
    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: |
          .uv/cache
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    - name: Install dependencies
      run: uv sync
    - name: Run unit tests
      run: uv run pytest tests/unit/ --junitxml=unit-results.xml
      continue-on-error: false
    - name: Upload unit test results
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results
        path: unit-results.xml

  integration-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
      LANGSMITH_TRACING: ${{ secrets.LANGSMITH_TRACING }}
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install uv
      uses: astral-sh/setup-uv@v1
      with:
        version: "latest"
    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: |
          .uv/cache
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    - name: Install dependencies
      run: uv sync
    - name: Run integration tests
      run: uv run pytest tests/integrations/ --junitxml=integration-results.xml
      continue-on-error: false
    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      with:
        name: integration-test-results
        path: integration-results.xml



  evaluation-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
      LANGSMITH_TRACING: ${{ secrets.LANGSMITH_TRACING }}
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install uv
      uses: astral-sh/setup-uv@v1
      with:
        version: "latest"
    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: |
          .uv/cache
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    - name: Install dependencies
      run: uv sync
    - name: Run evaluation tests
      run: uv run pytest -m evaluator --junitxml=evaluator-results.xml
      continue-on-error: false
    - name: Upload evaluation test results
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-test-results
        path: evaluator-results.xml

  e2e-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11"]
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
      LANGSMITH_TRACING: ${{ secrets.LANGSMITH_TRACING }}
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install uv
      uses: astral-sh/setup-uv@v1
      with:
        version: "latest"
    - name: Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: |
          .uv/cache
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    - name: Install dependencies
      run: uv sync
    - name: Run e2e tests
      run: uv run pytest tests/e2e/ --junitxml=e2e-results.xml
      continue-on-error: false
    - name: Upload e2e test results
      uses: actions/upload-artifact@v4
      with:
        name: e2e-test-results
        path: e2e-results.xml

  report-results:
    needs: [unit-tests, integration-tests, evaluation-tests, e2e-tests]
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    continue-on-error: true
    steps:
    - name: Download unit test results
      uses: actions/download-artifact@v4
      with:
        name: unit-test-results
    - name: Download integration test results
      uses: actions/download-artifact@v4
      with:
        name: integration-test-results
    - name: Download evaluation test results
      uses: actions/download-artifact@v4
      with:
        name: evaluation-test-results
    - name: Download e2e test results
      uses: actions/download-artifact@v4
      with:
        name: e2e-test-results
    - name: Parse test results
      id: parse-results
      run: |
        # Check if files exist and count tests
        if [ -f unit-results.xml ]; then
          UNIT_TESTS=$(grep -c '<testcase' unit-results.xml || echo '0')
        else
          UNIT_TESTS='0'
        fi
        
        if [ -f integration-results.xml ]; then
          INTEGRATION_TESTS=$(grep -c '<testcase' integration-results.xml || echo '0')
        else
          INTEGRATION_TESTS='0'
        fi
        
        if [ -f evaluator-results.xml ]; then
          EVALUATOR_TESTS=$(grep -c '<testcase' evaluator-results.xml || echo '0')
        else
          EVALUATOR_TESTS='0'
        fi
        
        if [ -f e2e-results.xml ]; then
          E2E_TESTS=$(grep -c '<testcase' e2e-results.xml || echo '0')
        else
          E2E_TESTS='0'
        fi
        
        echo "unit_passed=$UNIT_TESTS" >> $GITHUB_OUTPUT
        echo "integration_passed=$INTEGRATION_TESTS" >> $GITHUB_OUTPUT
        echo "evaluator_passed=$EVALUATOR_TESTS" >> $GITHUB_OUTPUT
        echo "e2e_passed=$E2E_TESTS" >> $GITHUB_OUTPUT
    - name: Comment PR with detailed results
      uses: actions/github-script@v7
      with:
        script: |
          const unitPassed = '${{ steps.parse-results.outputs.unit_passed }}';
          const integrationPassed = '${{ steps.parse-results.outputs.integration_passed }}';
          const evaluatorPassed = '${{ steps.parse-results.outputs.evaluator_passed }}';
          const e2ePassed = '${{ steps.parse-results.outputs.e2e_passed }}';
          
          // Read XML files and format them for display
          const fs = require('fs');
          
          let xmlSections = '';
          
          // Add unit test results
          if (fs.existsSync('unit-results.xml')) {
            const unitXml = fs.readFileSync('unit-results.xml', 'utf8');
            xmlSections += `\n\nüìã **Unit Test Results:**\n\`\`\`xml\n${unitXml}\n\`\`\``;
          }
          
          // Add integration test results
          if (fs.existsSync('integration-results.xml')) {
            const integrationXml = fs.readFileSync('integration-results.xml', 'utf8');
            xmlSections += `\n\nüìã **Integration Test Results:**\n\`\`\`xml\n${integrationXml}\n\`\`\``;
          }
          
          // Add evaluator test results
          if (fs.existsSync('evaluator-results.xml')) {
            const evaluatorXml = fs.readFileSync('evaluator-results.xml', 'utf8');
            xmlSections += `\n\nüìã **Evaluation Test Results:**\n\`\`\`xml\n${evaluatorXml}\n\`\`\``;
          }
          
          // Add e2e test results
          if (fs.existsSync('e2e-results.xml')) {
            const e2eXml = fs.readFileSync('e2e-results.xml', 'utf8');
            xmlSections += `\n\nüìã **E2E Test Results:**\n\`\`\`xml\n${e2eXml}\n\`\`\``;
          }
          
          const testResults = `üß™ **Test Results for Python 3.11**
          
          üìä **Test Summary:**
          - Unit Tests: ${unitPassed} passed
          - Integration Tests: ${integrationPassed} passed
          - E2E Tests: ${e2ePassed} passed
          - Offline Evaluations: ${evaluatorPassed} passed
          
          üîç **Details:**
          - Python Version: 3.11
          - Runner: ${{ runner.os }}
          - Commit: ${{ github.sha }}
          
          ${unitPassed > 0 && integrationPassed > 0 ? '‚úÖ All tests passed!' : '‚ùå Some tests failed. Check the logs for details.'}${xmlSections}`;
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: testResults
          }); 