import json

import pytest
from langsmith import Client
from openevals.llm import create_llm_as_judge

from agents.simple_text2sql import generate_sql, llm
from agents.utils import get_detailed_table_info

# Setup LangSmith client
client = Client()

# Define evaluators using OpenEvals
SQL_CORRECTNESS_PROMPT = """
You are an expert professor specialized in grading SQL queries generated by text2sql agents.

You are grading the following SQL generation:

Question: {inputs}
Expected SQL: {reference_outputs}
Generated SQL: {outputs}

Respond with CORRECT or INCORRECT.
CORRECT means the generated SQL is correct and matches the expected SQL or is close to it.
INCORRECT means the generated SQL is incorrect and does not match the expected SQL.

Grade:
"""

SQL_QUALITY_PROMPT = """
Rate the quality of this SQL query on a scale of 1-5:

Question: {inputs}
Expected SQL: {reference_outputs}
Generated SQL: {outputs}

Rate the SQL quality (1-5):
1. Completely incorrect SQL (syntax errors, wrong tables/columns)
2. Partially correct but has significant issues
3. Mostly correct with minor problems
4. Correct SQL that should work
5. Perfect SQL that matches the question exactly

Rating:
"""

sql_correctness_evaluator = create_llm_as_judge(
    prompt=SQL_CORRECTNESS_PROMPT,
    feedback_key="sql_correctness",
    model="openai:gpt-4o-mini",
)

sql_quality_evaluator = create_llm_as_judge(
    prompt=SQL_QUALITY_PROMPT,
    feedback_key="sql_quality",
    model="openai:gpt-4o-mini",
)


def ls_sql_target(inputs: dict) -> dict:
    """LangSmith target function that generates SQL using the agent"""
    from langchain_core.messages import HumanMessage

    sql_generator = generate_sql(llm)

    # Mock state for SQL generation
    state = {
        "messages": [HumanMessage(content=inputs["question"])],
        "schema": get_detailed_table_info(),
        "sql": "",
        "records": [],
    }

    result = sql_generator(state)
    return {"sql": result["sql"]}


@pytest.mark.evaluator
def test_sql_generation_evaluation():
    """Run SQL generation evaluation using LangSmith"""

    # Run evaluation
    experiment_results = client.evaluate(
        ls_sql_target,  # Your SQL generation system
        data="text2sql-agent",  # The dataset to predict and grade over
        evaluators=[
            sql_correctness_evaluator,
            sql_quality_evaluator,
        ],  # The evaluators to score the results
        max_concurrency=10,
        experiment_prefix="text2sql-agent-sql",  # A prefix for your experiment names
    )

    assert experiment_results is not None
    print(f"âœ… Evaluation completed: {experiment_results.experiment_name}")

    # Define scoring rules
    criteria = {"sql_correctness": ">=0.75", "sql_quality": ">=3.0"}

    output_metadata = {
        "experiment_name": experiment_results.experiment_name,
        "criteria": criteria,
    }

    safe_name = experiment_results.experiment_name.replace(":", "-").replace("/", "-")
    config_filename = f"evaluation_config__{safe_name}.json"
    with open(config_filename, "w") as f:
        json.dump(output_metadata, f)

    print(f"::set-output name=config_filename::{config_filename}")
