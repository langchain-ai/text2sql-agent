import pytest
from langsmith import Client
from openevals.llm import create_llm_as_judge
from agents.simple_text2sql import generate_sql, llm
from agents.utils import get_detailed_table_info

# Setup LangSmith client
client = Client()

# Define evaluators using OpenEvals
SQL_CORRECTNESS_PROMPT = """
You are an expert professor specialized in grading SQL queries generated by text2sql agents.

You are grading the following SQL generation:

Question: {inputs}
Expected SQL: {reference_outputs}
Generated SQL: {outputs}

Respond with CORRECT or INCORRECT.
CORRECT means the generated SQL is correct and matches the expected SQL or is close to it.
INCORRECT means the generated SQL is incorrect and does not match the expected SQL.

Grade:
"""

SQL_QUALITY_PROMPT = """
Rate the quality of this SQL query on a scale of 1-5:

Question: {inputs}
Expected SQL: {reference_outputs}
Generated SQL: {outputs}

Rate the SQL quality (1-5):
1. Completely incorrect SQL (syntax errors, wrong tables/columns)
2. Partially correct but has significant issues
3. Mostly correct with minor problems
4. Correct SQL that should work
5. Perfect SQL that matches the question exactly

Rating:
"""

sql_correctness_evaluator = create_llm_as_judge(
    prompt=SQL_CORRECTNESS_PROMPT,
    feedback_key="sql_correctness",
    model="openai:gpt-4o-mini",
)

sql_quality_evaluator = create_llm_as_judge(
    prompt=SQL_QUALITY_PROMPT,
    feedback_key="sql_quality",
    model="openai:gpt-4o-mini",
)

def ls_sql_target(inputs: dict) -> dict:
    """LangSmith target function that generates SQL using the agent"""
    from langchain_core.messages import HumanMessage
    
    sql_generator = generate_sql(llm)
    
    # Mock state for SQL generation
    state = {
        "messages": [HumanMessage(content=inputs["question"])],
        "schema": get_detailed_table_info(),
        "sql": "",
        "records": []
    }
    
    result = sql_generator(state)
    return {"sql": result["sql"]}

@pytest.mark.evaluator
def test_sql_generation_evaluation():
    """Run SQL generation evaluation using LangSmith"""
    
    # Run evaluation
    experiment_results = client.evaluate(
        ls_sql_target,  # Your SQL generation system
        data="text2sql-agent",  # The dataset to predict and grade over
        evaluators=[sql_correctness_evaluator, sql_quality_evaluator],  # The evaluators to score the results
        max_concurrency=10,
        experiment_prefix="text2sql-agent-sql",  # A prefix for your experiment names
    )
    
    assert experiment_results is not None
    print(f"SQL Generation Evaluation completed. Results: {experiment_results}")
    
    # Fetch feedback scores and assert minimum thresholds
    feedback = client.list_feedback(
        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],
    )
    
    # Test SQL correctness scores (boolean - should be at least 75% correct)
    sql_correctness_feedback = [f for f in feedback if f.key == 'sql_correctness']
    sql_correctness_scores = [f.score for f in sql_correctness_feedback if f.score is not None]
    if sql_correctness_scores:
        sql_correctness_percentage = sum(sql_correctness_scores) / len(sql_correctness_scores)
        print(f"SQL Correctness Score: {sql_correctness_percentage:.2%} ({sum(sql_correctness_scores)}/{len(sql_correctness_scores)})")
        assert sql_correctness_percentage >= 0.75, f"SQL correctness score {sql_correctness_percentage:.2%} is below 75% threshold"
    
    # Test SQL quality scores (1-5 scale - should be at least 3.0 average)
    sql_quality_feedback = [f for f in feedback if f.key == 'sql_quality']
    sql_quality_scores = [f.score for f in sql_quality_feedback if f.score is not None]
    if sql_quality_scores:
        sql_quality_average = sum(sql_quality_scores) / len(sql_quality_scores)
        print(f"SQL Quality Score: {sql_quality_average:.2f}/5 (min: {min(sql_quality_scores)}, max: {max(sql_quality_scores)})")
        assert sql_quality_average >= 3.0, f"SQL quality score {sql_quality_average:.2f} is below 3.0 threshold"
    
    print(f"âœ… All SQL evaluation thresholds met! Processed {len(sql_correctness_scores)} correctness and {len(sql_quality_scores)} quality scores.")
    
    # Write evaluation results to file for GitHub Actions parsing
    import json
    import os
    
    eval_results = {
        "sql_correctness": sql_correctness_percentage if sql_correctness_scores else 0.0,
        "sql_quality": sql_quality_average if sql_quality_scores else 0.0,
        "sql_correctness_passed": sql_correctness_percentage >= 0.75 if sql_correctness_scores else False,
        "sql_quality_passed": sql_quality_average >= 3.0 if sql_quality_scores else False
    }
    
    # Write to file that GitHub Actions can read
    with open("sql_evaluation_results.json", "w") as f:
        json.dump(eval_results, f) 